# ğŸ”¥ LLM Wrapper: Smart API Router for Multi-Model AI Responses

## ğŸš€ Overview
**LLM Wrapper** is an intelligent routing system that dynamically selects between **Gemini 1.5 Flash** and **Meta Llama 3.3 70B Instruct Turbo Free** models based on prompt classification. It ensures optimal responses by leveraging:
- **Gemini** for **factual/textual** queries.
- **Llama 3.3** for **logical/mathematical** tasks.

## âœ¨ Features
âœ… **Automatic Prompt Classification** â€“ Determines whether a query is factual or logical.  
âœ… **Multi-Model Support** â€“ Routes prompts to the best-suited model (**Gemini or Llama 3.3**).  
âœ… **Efficient API Calls** â€“ Minimizes unnecessary requests, reducing latency.  
âœ… **Retry Mechanism** â€“ Handles failures with automatic retries.  
âœ… **Scalable & Modular** â€“ Easy to extend with more models.  

## ğŸ› ï¸ Tech Stack
- **Python** â€“ Core programming language.
- **Google Generative AI SDK** â€“ For calling Gemini 1.5 Flash.
- **Together API** â€“ For interacting with Meta Llama 3.3 70B Instruct Turbo Free.
- **Environment Variables** â€“ Secure API key storage.

## ğŸ“¦ Installation
1ï¸âƒ£ Clone the repository and install dependencies:
   ```sh
   git clone https://github.com/your-username/llm-wrapper.git
   cd llm-wrapper
   pip install -r requirements.txt
```


2ï¸âƒ£ Set up your API keys as environment variables:
```sh
export GEMINI_API_KEY="your_google_api_key"
export TOGETHER_API_KEY="your_together_api_key"
```

## ğŸš€ Usage

Import and use the LLM Wrapper in your Python script:

from llm_wrapper import LLMWrapper

wrapper = LLMWrapper()
prompt = "Explain the concept of black holes."
response = wrapper.get_response(prompt)
print(response)

## ğŸ—ï¸ Project Structure
```
llm-wrapper/
â”‚â”€â”€ llm_wrapper/
â”‚   â”‚â”€â”€ __init__.py       # Package initialization
â”‚   â”‚â”€â”€ classifier.py     # Classifies prompts as factual or logical
â”‚   â”‚â”€â”€ llm_client.py     # Handles API calls to Gemini and Llama 3.3
â”‚â”€â”€ main.py               # Example script for testing the wrapper
â”‚â”€â”€ requirements.txt      # Python dependencies
â”‚â”€â”€ README.md             # Project documentation
```
## ğŸ”¥ Future Enhancements

Add support for more LLMs (GPT-4, Claude, Mistral, etc.).

Improve classification accuracy using ML-based techniques.

Implement caching to optimize repeated queries.

## ğŸ¤ Contributing

Contributions are welcome! Feel free to submit a pull request or open an issue.

## ğŸ“œ License

This project is licensed under the MIT License. See LICENSE for details.
